from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Input, concatenate
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
from keras.models import Model
import os
import tarfile
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing import sequence
from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model, Sequential
from keras.layers import Convolution1D
from keras import initializers, regularizers, constraints, optimizers, layers
import nltk
from nltk.corpus import sentiwordnet as swn
import pandas as pd
import numpy as np
import re
import string
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics
from sklearn.metrics import accuracy_score
import sklearn
import pandas as pd
import time

#进行词性还原
wnl = WordNetLemmatizer()
#停用词表
stopword_list = nltk.corpus.stopwords.words('english')

#分词
def tokenize_text(text):
    tokens = nltk.word_tokenize(text)
    tokens = [token.strip() for token in tokens]
    return tokens

#去掉停用词
def remove_stopwords(text):
    tokens = tokenize_text(text)
    filtered_tokens = [token for token in tokens if token not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text

#去掉特殊字符
def remove_special_characters(text):
    tokens = tokenize_text(text)
    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))
    filtered_tokens = filter(None, [re.sub(pattern, '', token) for token in tokens])
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text

#词性标注
def pos_tag_text(text):
    # convert penn treebank tag to wordnet tag
    def penn_to_wn_tags(pos_tag):
        if pos_tag.startswith('J'):
            return wn.ADJ
        elif pos_tag.startswith('V'):
            return wn.VERB
        elif pos_tag.startswith('N'):
            return wn.NOUN
        elif pos_tag.startswith('R'):
            return wn.ADV
        else:
            return None
    tokens = nltk.word_tokenize(text)
    tagged_tokens = nltk.pos_tag(tokens)
    tagged_lower_tokens = [(word.lower(), penn_to_wn_tags(pos_tag)) for word, pos_tag in tagged_tokens]
    return tagged_lower_tokens

#词形还原
def lemmatize_text(text):
    # lemmatize text based on pos tags
    pos_tagged_text = pos_tag_text(text)
    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag
                         else word
                         for word, pos_tag in pos_tagged_text]
    lemmatized_text = ' '.join(lemmatized_tokens)
    return lemmatized_text

#数据预处理
def text_normalize(corpus, tokenize=False, lemmatize=False, stopword=True):
    normalized_corpus = []
    for text in corpus:
        text = text.lower()
        if lemmatize:
            text = lemmatize_text(text)
        text = remove_special_characters(text)
        if stopword:
            text = remove_stopwords(text)
        if tokenize:
            text = tokenize_text(text)
        normalized_corpus.append(text)
    return normalized_corpus


f=open('D:\movie_reviews.csv',encoding='gb18030')
dataset = pd.read_csv(f)
#划分训练集和测试集
train_data = dataset[:25000]
test_data = dataset[25000:]
# print(train_data.iloc[:,0])
# print(test_data)
# train_corpus=train_data.iloc[:,0]
# test_corpus=test_data.iloc[:,0]
# train_labels=train_data.iloc[:,1]
# test_labels=test_data.iloc[:,1]
train_corpus = np.array(train_data['review'])
train_labels = np.array(train_data['sentiment'])
test_corpus = np.array(test_data['review'])
test_labels = np.array(test_data['sentiment'])
norm_train_corpus = text_normalize(train_corpus)#对数据进行标准化处理
norm_test_corpus = text_normalize(test_corpus)

n_norm = norm_train_corpus.copy()
content=n_norm.extend(norm_test_corpus)
content=n_norm

max_feature = 6000
tokenizer = Tokenizer(max_feature)#可以设置max_feature,即表示最长的
tokenizer.fit_on_texts(norm_train_corpus)

maxlen = 200
# 获取词索引
word_index=tokenizer.word_index
norm_train_corpus = tokenizer.texts_to_sequences(norm_train_corpus)
norm_train_corpus =sequence.pad_sequences(norm_train_corpus,maxlen=maxlen)

norm_test_corpus = tokenizer.texts_to_sequences(norm_test_corpus)
norm_test_corpus =sequence.pad_sequences(norm_test_corpus,maxlen=maxlen)


ytrain = []
for i in train_labels:
    if i =="positive":
        ytrain.append(1)    
    else:
        ytrain.append(0)      
ytest = []
for i in test_labels:
    if i =="positive":
        ytest.append(1)    
    else:
        ytest.append(0)  
Xtrain = norm_train_corpus
Xtest = norm_test_corpus

def text_lstm(maxlen=150, max_features=2000, embed_size=32):
    model=Sequential()
    model.add(Embedding(max_feature,256,input_length=200))
    model.add(LSTM(128,dropout=0.2))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))
    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
    model.summary()
    return model

def bi_lstm(maxlen=150, max_features=2000, embed_size=32):
    model = Sequential()
    model.add(Embedding(max_feature, embed_size))#在tokenizer里设置好max_feature后，可以设置好对应的embed
    model.add(Bidirectional(LSTM(32, return_sequences = True)))
    model.add(GlobalMaxPool1D())
    model.add(Dense(20, activation="relu"))
    model.add(Dropout(0.05))
    model.add(Dense(1, activation="sigmoid"))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model


def text_cnn(maxlen=150, max_features=2000, embed_size=32):
    # Inputs
    comment_seq = Input(shape=[maxlen], name='x_seq')

    # Embeddings layers
    emb_comment = Embedding(max_features, embed_size)(comment_seq)

    # conv layers
    convs = []
    filter_sizes = [2, 3, 4, 5]
    for fsz in filter_sizes:
        l_conv = Conv1D(filters=100, kernel_size=fsz, activation='relu')(emb_comment)
        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)
        l_pool = Flatten()(l_pool)
        convs.append(l_pool)
    merge = concatenate(convs, axis=1)

    out = Dropout(0.5)(merge)
    output = Dense(32, activation='relu')(out)

    output = Dense(units=1, activation='sigmoid')(output)

    model = Model([comment_seq], output)
    #     adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
    model.compile(loss="binary_crossentropy", optimizer="adam", metrics=['accuracy'])
    model.summary()
    return model
#text-cnn
#accuracy: 0.8609
#precision: 0.8335
#recall: 0.9027
#f1 score: 0.8667

def mlp_create_model(maxlen=150, max_features=2000, embed_size=32):
    model = Sequential()
    # 构建嵌入层
    model.add(Embedding(max_features, output_dim=embed_size, input_length=maxlen))
    model.add(Flatten())
    model.add(Dense(250, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    return model
#mlp
#accuracy: 0.8743
#precision: 0.8733
#recall: 0.8763
#f1 score: 0.8748
#totally cost 101.6441740989685

def hist_t_f(model):
    hist = model.fit(Xtrain, ytrain,
              validation_split=0.1,
              batch_size=batch_size,
              epochs=epochs,
              shuffle=True)
    scores = model.evaluate(Xtest, ytest)
    print('test_loss: %f, accuracy: %f' % (scores[0], scores[1]))
    y_pre = model.predict(Xtest)
    y_pre = (y_pre>0.5)##得到结果是一个概率值
    from sklearn import metrics
    accuracy=np.round(metrics.accuracy_score(ytest,y_pre),4)
    precision=np.round(metrics.precision_score(ytest,y_pre),4)
    recall=np.round(metrics.recall_score(ytest,y_pre),4)
    f1=np.round(metrics.f1_score(ytest,y_pre),4)
    print("accuracy:", accuracy)
    print("precision:", precision)
    print("recall:", recall)
    print("f1 score:", f1)
    return hist


import time
model1 = text_lstm(200,6000)
model2 = bi_lstm(200,6000)
model3 = text_cnn(200,6000)
model4 = mlp_create_model(200,6000)

batch_size = 128
epochs = 20
time_start = time.time()
hist1 = hist_t_f(model1)
hist2 = hist_t_f(model2)
hist3 = hist_t_f(model3)
hist4 = hist_t_f(model4)
time_end = time.time()
print('totally cost',time_end-time_start)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve #导入ROC曲线函数
def plot_roc(hist):
    plt.figure()
    acc = hist.history['acc']
    val_acc = hist.history['val_acc']
    loss = hist.history['loss']
    val_loss = hist.history['val_loss']
    #f1_score1 = hist1.history['f1_score']
      
    epochs = range(len(acc))
    
    #绘制训练集和验证集的准确率对比
    plt.plot(epochs, acc, 'bo', label='Training acc') # 'bo'为画蓝色圆点，不连线
    plt.plot(epochs, val_acc, 'b', label='Validation acc') 
    plt.title('text-mlp_Training and validation accuracy')
    plt.legend() # 绘制图例，默认在右上角
     
    plt.figure()
    #绘制训练集和验证集的损失对比
    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('text-mlp_Training and validation loss')
    plt.legend()
    plt.show()
    
plot_roc(hist1)
plot_roc(hist2)
plot_roc(hist3)
plot_roc(hist4)
